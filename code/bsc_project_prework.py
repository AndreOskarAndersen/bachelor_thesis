# -*- coding: utf-8 -*-
"""bsc_project_prework.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ONiblMVSwbXZGBBbGLVECNz9bWZC6hf7
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import skimage.io as io
from pycocotools.coco import COCO # DOC: https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/coco.py
import pylab
from tqdm.notebook import tqdm
import skimage
import skimage.draw
import skimage.transform
import skimage.filters
from sklearn.model_selection import train_test_split
import h5py
pylab.rcParams['figure.figsize'] = (8.0, 10.0)

!rm -r /content/data/
!rm validation_images.zip

!mkdir /content/data
!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -p /content
!unzip /content/images.cocodataset.org/annotations/annotations_trainval2017.zip -d /content/data/annotations_trainval2017
!mv /content/data/annotations_trainval2017/annotations /content/data/annotations
!rm -r /content/data/annotations_trainval2017
!rm -r /content/images.cocodataset.org

def draw_keypoints(I, keypoints, r = 0.75, g = 0.6, b = 0.2, radius = 2):
  """Draws keypoints on image"""
  I_copy = np.copy(I)
  x_val = keypoints[::3]
  y_val = keypoints[1::3]
  v_val = keypoints[2::3]

  for x, y, v in zip(x_val, y_val, v_val):
    if (v != 0):
      rr, cc = skimage.draw.circle(y, x, radius)
      I_copy[rr - 1, cc - 1] = [r, g, b]

  return I_copy

def draw_bbox(I, bbox, r, g, b):
  I_copy = np.copy(I)

  x_min = np.ceil(bbox[0]).astype("int")
  x_max = np.floor(bbox[0] + bbox[2]).astype("int")
  y_min = np.ceil(bbox[1]).astype("int")
  y_max = np.floor(bbox[1] + bbox[3]).astype("int")

  rr, cc = skimage.draw.line(y_min, x_min, y_min, x_max)
  I_copy[rr - 1, cc - 1] = [r, g, b]

  rr, cc = skimage.draw.line(y_max, x_min, y_max, x_max)
  I_copy[rr - 1, cc - 1] = [r, g, b]

  rr, cc = skimage.draw.line(y_min, x_min, y_max, x_min)
  I_copy[rr - 1, cc - 1] = [r, g, b]

  rr, cc = skimage.draw.line(y_min, x_max, y_max, x_max)
  I_copy[rr - 1, cc - 1] = [r, g, b]

  return I_copy

def draw_segmentation_extremes(I, segmentation, r, g, b, radius):
  assert len(segmentation) == 1

  I_copy = np.copy(I)
  segmentation = np.array(segmentation[0]).reshape((-1, 2))
  x_min, y_min = np.min(segmentation, axis = 0)[0], np.min(segmentation, axis = 0)[1]
  x_max, y_max = np.max(segmentation, axis = 0)[0], np.max(segmentation, axis = 0)[1]

  rr, cc = skimage.draw.circle(y_min, x_min, radius)
  I_copy[rr - 1, cc - 1] = [r, g, b]

  rr, cc = skimage.draw.circle(y_max, x_min, radius)
  I_copy[rr - 1, cc - 1] = [r, g, b]

  rr, cc = skimage.draw.circle(y_min, x_max, radius)
  I_copy[rr - 1, cc - 1] = [r, g, b]

  rr, cc = skimage.draw.circle(y_max, x_max, radius)
  I_copy[rr - 1, cc - 1] = [r, g, b]

  return I_copy

def crop_image(I, anns, margin_size):
  """Crops images"""
  res_images = []
  res_keypoints = []
  success_arr = np.array([]) # indicates if the image were cropped correctly. 1 if correctly, 0 if incorrectly

  # For each person in the image
  for person in range(len(anns)):  

    # USED FOR DEBUGGING
    I_copy = np.copy(I)

    keypoints = np.array(anns[person]["keypoints"])
    bbox = anns[person]["bbox"]
    x_val = np.array(keypoints[::3]) - 1 # COCO is not 0-indexed
    y_val = np.array(keypoints[1::3]) - 1 # COCO is not 0-indexed
    v_val = keypoints[2::3]

    # If the current person is not annotated with keypoints
    if (int(np.sum(v_val)) == 0):
      res_images.append(np.zeros((1, 1)))
      res_keypoints.append(np.zeros(len(keypoints)))
      success_arr = np.append(success_arr, 0)
      print("PERSON NOT ANNOTATED")
      continue

    # Finding edges of bounding box
    x_min = np.floor(bbox[0]).astype("int")
    y_min = np.floor(bbox[1]).astype("int")
    x_max = np.ceil(bbox[0] + bbox[2]).astype("int")
    y_max = np.ceil(bbox[1] + bbox[3]).astype("int")

    # If bounding box does not contain every keypoint, expand it so it does
    keypoints = keypoints.reshape((-1, 3))
    filtered_keypoints = keypoints[keypoints[:, -1] != 0] # keypoints where v != 0
    min_keypoints = np.min(filtered_keypoints, axis = 0)
    max_keypoints = np.max(filtered_keypoints, axis = 0)
    x_min_kp = min_keypoints[0]
    x_max_kp = max_keypoints[0]
    y_min_kp = min_keypoints[1]
    y_max_kp = max_keypoints[1]
    x_min = min(x_min, x_min_kp)
    y_min = min(y_min, y_min_kp)
    x_max = max(x_max, x_max_kp)
    y_max = max(y_max, y_max_kp)

    # If it is not possible for the image to be centered wrt the x-axis
    if (x_min_kp + (x_max_kp - x_min_kp)/2 - (y_max_kp - y_min_kp)/2 < 0 or x_min_kp + (x_max_kp - x_min_kp)/2 + (y_max_kp - y_min_kp)/2 > I.shape[1]):
      res_images.append(np.zeros((1, 1)))
      res_keypoints.append(np.zeros(len(keypoints)))
      success_arr = np.append(success_arr, 0)
      print("not possible")
      continue

     # Makes the image have squared aspect ratio
    if (x_max - x_min > y_max - y_min):
      additional = 1/2 * ((x_max - x_min) - (y_max - y_min))
      y_max += np.ceil(additional).astype("int")
      y_min -= np.floor(additional).astype("int")
    elif (y_max - y_min > x_max - x_min):
      additional = 1/2 * ((y_max - y_min) - (x_max - x_min))
      x_max += np.ceil(additional).astype("int")
      x_min -= np.floor(additional).astype("int")

    # Centers the bounding box around all of the keypoints of the person
    old_x_min = x_min
    old_x_max = x_max
    center = [x_min_kp + (x_max_kp - x_min_kp)/2, y_min_kp + (y_max_kp - y_min_kp)/2]
    x_kp_center_coor = center[0]
    curr_x_center_coor = x_min + (x_max - x_min)/2
    curr_y_center_coor = y_min + (y_max - y_min)/2
    new_x_min = x_min + np.floor(x_kp_center_coor - curr_x_center_coor).astype("int")
    new_x_max = x_max + np.floor(x_kp_center_coor - curr_x_center_coor).astype("int")
    x_min = new_x_min
    x_max = new_x_max

    # Resizes the bbox
    y_dist = y_max_kp - y_min_kp
    x_dist = x_max_kp - x_min_kp
    side_length = max(y_dist, x_dist) * (1 + margin_size)

    if (side_length != 0):
      x_side_diff = (x_max - x_min) - side_length
      x_min += int(x_side_diff//2) 
      x_max -= int(x_side_diff//2)

    # If the bbox with the margins does not fit in the image, shrink the bbox
    if (x_min < 0):
      x_max += -1 * (x_min)
      y_max += -1 * (x_min)
      y_min += x_min
      x_min = 0
    elif (x_max > I.shape[1]):
      diff = x_max - I.shape[1]
      x_max = I.shape[1]
      x_min += diff
      y_min += diff
      y_max -= diff
    
    y_side_diff = (y_max - y_min) - side_length
    y_min += int(y_side_diff//2)
    y_max -= int(y_side_diff//2)

    if (side_length < x_max_kp - x_min_kp or side_length < y_max_kp - y_min_kp): # if the bbox cannot fit the keypoints, discard this person
      res_images.append(np.zeros((1, 1)))
      res_keypoints.append(np.zeros(len(keypoints)))
      success_arr = np.append(success_arr, 0)
      print("does not fit")
      continue

    # Moving the bbox up or down untill it fits all of the keypoints. If this is not possible, discard the image
    y_kp_center_coor = center[1]
    new_y_min = y_min + np.floor(y_kp_center_coor - curr_y_center_coor).astype("int")
    new_y_max = y_max + np.floor(y_kp_center_coor - curr_y_center_coor).astype("int")
    y_min = new_y_min
    y_max = new_y_max

    if (y_min < 0):
      y_max += -1 * (y_min)
      y_min = 0
    elif (y_max > I.shape[0]):
      y_min -= (y_max - I.shape[0])
      y_max = I.shape[0]

    I_cropped = I[y_min : y_max, x_min : x_max]

    # Moving keypoints
    x_val = np.array(x_val) - x_min
    y_val = np.array(y_val) - y_min
      
    # Combining the keypoints into one array again
    temp_keypoints = []
    for t in zip(x_val, y_val, v_val):
      for i in t:
        temp_keypoints.append(i)

    # Casting to np.arrays
    I_cropped = np.array(I_cropped)
    temp_keypoints = np.array(temp_keypoints)

    # Appending the cropped image and the new keypoints to the results
    res_images.append(I_cropped)
    res_keypoints.append(temp_keypoints)

    if (I_cropped.shape[0] == I_cropped.shape[1]):
      success_arr = np.append(success_arr, 1)
    else:
      success_arr = np.append(success_arr, 0)
      print("WRONG DIMENSIONS")

  return res_images, res_keypoints, success_arr

def resize_img(I, keypoints, wanted_width, wanted_height):
  """Resizes image img and keypoints keypoints to have size wanted_height x wanted_width"""
  I_resized = skimage.transform.resize(I, (wanted_width, wanted_height))

  # Extracting keypoints coordinates
  x_val = np.array(keypoints[::3]) * wanted_width//I.shape[1]
  y_val = np.array(keypoints[1::3]) * wanted_height//I.shape[0]
  v_val = np.array(keypoints[2::3])

  # Combining the keypoints into one array again
  keypoints_resize = []
  for t in zip(x_val, y_val, v_val):
    for i in t:
      keypoints_resize.append(i)

  assert I_resized.shape[0] == wanted_height and I_resized.shape[1] == wanted_width

  return I_resized, keypoints_resize

def prepare_image(I, anns, wanted_width, wanted_height, margin_size):
  """Crops and resizes a single image containing people"""

  res_img, res_kp = [], []
  cropped_images, cropped_keypoints, success_arr = crop_image(I, anns, margin_size)

  for image, keypoints, succ in zip(cropped_images, cropped_keypoints, success_arr):
    if succ:
      resized_img, resized_kp = resize_img(image, keypoints,  wanted_width, wanted_height)
      res_img.append(resized_img)
      res_kp.append(resized_kp)
    else:
      res_img.append(image)
      res_kp.append(keypoints)

  return res_img, np.array(res_kp), success_arr

def prepare_output(img_shape, wanted_width, wanted_height, keypoints):
  """Creates the 17 (or len(keypoints)/3) heatmaps corresponding to each keypoint of a single image with size wanted_height x wanted_width"""

  # Finds boundingbox
  x_val = keypoints[::3]
  y_val = keypoints[1::3]
  v_val = keypoints[2::3]

  # The 17 results are stored here
  res_arr = []

  for x, y, v in zip(x_val, y_val, v_val):
    x, y, v = int(x), int(y), int(v)
    res = np.zeros((img_shape[0], img_shape[1]))
    res[y - 1, x - 1] = 1

    res = skimage.transform.resize(res, (wanted_height, wanted_width))
    if (v == 1):
      res = skimage.filters.gaussian(res, sigma = 1)
    elif (v == 2):
      res = skimage.filters.gaussian(res, sigma = 0.5)

    res_arr.append(res)

  return res_arr

""" Saving validation images """

!rm -r /content/data/validation_images
!rm -r /content/data/validation_images/input
!rm -r /content/data/validation_images/output

!mkdir /content/data/validation_images
!mkdir /content/data/validation_images/input
!mkdir /content/data/validation_images/output

# Load all images in val2017
annFile_instances = "/content/data/annotations/instances_val2017.json"
coco_instances = COCO(annFile_instances)

catIds = coco_instances.getCatIds(catNms = ['person']) # Gets category ids with a person in it
imgIds = coco_instances.getImgIds(catIds=catIds) # Gets all images with a person in it

annFile_kp = "/content/data/annotations/person_keypoints_val2017.json"
coco_kps = COCO(annFile_kp) 

wanted_width, wanted_height = 256, 256
margin_size = 0.1

prep_anns = {}
average_rgb = []

# For each image in the validation set
for imgId in tqdm(imgIds):
  print(imgId)
  img_info = coco_instances.loadImgs(imgId)[0]
  annIds = coco_kps.getAnnIds(imgIds = img_info["id"], catIds = catIds)
  anns = coco_kps.loadAnns(annIds)
  img = io.imread(img_info['coco_url']) # Loads image
  prepared_imgs, prepared_anns, success_arr = prepare_image(img, anns, wanted_width, wanted_height, margin_size) # Prepares the image
  for i in range(len(prepared_imgs)):
    print(i)
    if (success_arr[i]):
      plt.imshow(draw_keypoints(prepared_imgs[i], prepared_anns[i], r=1, g=0, b=0, radius = 2))
      plt.show()
    else:
      print("FAILED ID: {}, i: {}".format(imgId, i))

"""
  # For each of the prepared images
  for i, (prep_img, prep_ann, succ) in enumerate(zip(prepared_imgs, prepared_anns, success_arr)):
    if (succ == 1): # If the image was successfully prepared
        path_input = "/content/data/validation_images/input/{}_{}.png".format(img_info["id"], i)
        io.imsave(path_input, (prep_img * 255).astype(np.uint8)) # Saves the prepared image
        average_rgb.append(prep_img)
        #outputs = prepare_output(prep_img.shape, prep_ann, 64, 64) # Finds the 17 outputs

        #for j, output in enumerate(outputs): # For each of the 17 outputs
        #  path_output = "/content/data/validation_images/output/{}_{}_{}.npy".format(img_info["id"], i, j)
        #  f = h5py.File(path_output, "w")
        #  f.create_dataset("default", data = output, dtype = output.dtype)
        #  f.close()

        prep_anns["{}_{}".format(img_info["id"], i)] = prep_ann

df = pd.DataFrame.from_dict(prep_anns)
df.to_csv("/content/data/validation_images/output.txt", index = False)

average_rgb = np.mean(average_rgb, axis = 0)
f = h5py.File("/content/data/validation_images/output/average_rgb.h5", w)
f.create_dataset("default", data = average_rgb, dtype = average_rgb.dtype)
f.close()

!zip -r /content/data/validation_images/input.zip /content/data/validation_images/input
#!zip -r /content/data/validation_images/output.zip /content/data/validation_images/output
"""

#!rm -r /content/data/validation_images/input
#!rm -r /content/data/validation_images/output

""" Preparing saving training and testing images """
!rm -r /content/data/training_images
!rm -r /content/data/test_images
!rm -r /content/data/training_images/input
!rm -r /content/data/training_images/output
!rm -r /content/data/test_images/input
!rm -r /content/data/test_images/output


!mkdir /content/data/training_images
!mkdir /content/data/test_images
!mkdir /content/data/training_images/input
!mkdir /content/data/training_images/output
!mkdir /content/data/test_images/input
!mkdir /content/data/test_images/output


# Load all images in val2017
annFile_instances = "/content/data/annotations/instances_train2017.json"
coco_instances = COCO(annFile_instances)

catIds = coco_instances.getCatIds(catNms = ['person']) # Gets category ids with a person in it
imgIds = coco_instances.getImgIds(catIds=catIds) # Gets all images with a person in it

imgIds_train, imgIds_test = train_test_split(imgIds, train_size = 0.8, random_state = True)

"""Saving training images"""
annFile_kp = "/content/data/annotations/person_keypoints_train2017.json"
coco_kps = COCO(annFile_kp) 

wanted_width, wanted_height = 128, 128

prep_anns = {}

for imgId in tqdm(imgIds_train):
  img_info = coco_instances.loadImgs(imgId)[0]
  annIds = coco_kps.getAnnIds(imgIds = img_info["id"], catIds = catIds)
  anns = coco_kps.loadAnns(annIds)
  img = io.imread(img_info['coco_url'])
  prepared_imgs, prepared_anns, success_arr = prepare_image(img, anns, wanted_width, wanted_height)

  for i, (prep_img, prep_ann, succ) in enumerate(zip(prepared_imgs, prepared_anns, success_arr)):
    if (succ == 1): # If the image was successfully prepared
        path_input = "/content/data/training_images/input/{}_{}.png".format(img_info["id"], i)
        io.imsave(path_input, (prep_img * 255).astype(np.uint8)) # Saves the prepared image
        #outputs = prepare_output(prep_img.shape, prep_ann, 64, 64) # Finds the 17 outputs

        #for j, output in enumerate(outputs): # For each of the 17 outputs
        #  path_output = "/content/data/training_images/output/{}_{}_{}.npy".format(img_info["id"], i, j)
        #  f = h5py.File(path_output, "w")
        #  f.create_dataset("default", data = output, dtype = output.dtype)
        #  f.close()

        prep_anns["{}_{}".format(img_info["id"], i)] = prep_ann

df = pd.DataFrame.from_dict(prep_anns)
df.to_csv("/content/data/training_images/output.txt", index = False)


!zip -r /content/data/training_images/input.zip /content/data/training_images/input
#!zip -r /content/data/training_images/output.zip /content/data/training_images/output

!rm -r /content/data/training_images/input
!rm -r /content/data/training_images/output

"""Saving test images"""
"""
annFile_kp = "/content/data/annotations/person_keypoints_train2017.json"
coco_kps = COCO(annFile_kp) 

wanted_width, wanted_height = 128, 128

prep_anns = {}

for imgId in tqdm(imgIds_test):
  img_info = coco_instances.loadImgs(imgId)[0]
  annIds = coco_kps.getAnnIds(imgIds = img_info["id"], catIds = catIds)
  anns = coco_kps.loadAnns(annIds)
  img = io.imread(img_info['coco_url'])
  prepared_imgs, prepared_anns, success_arr = prepare_image(img, anns, wanted_width, wanted_height)

  for i, (prep_img, prep_ann, succ) in enumerate(zip(prepared_imgs, prepared_anns, success_arr)):
    if (succ == 1): # If the image was successfully prepared
        path_input = "/content/data/test_images/input/{}_{}.png".format(img_info["id"], i)
        io.imsave(path_input, (prep_img * 255).astype(np.uint8)) # Saves the prepared image
        #outputs = prepare_output(prep_img.shape, prep_ann, 64, 64) # Finds the 17 outputs

        #for j, output in enumerate(outputs): # For each of the 17 outputs
        #  path_output = "/content/data/test_images/output/{}_{}_{}.npy".format(img_info["id"], i, j)
        #  f = h5py.File(path_output, "w")
        #  f.create_dataset("default", data = output, dtype = output.dtype)
        #  f.close()

        prep_anns["{}_{}".format(img_info["id"], i)] = prep_ann

df = pd.DataFrame.from_dict(prep_anns)
df.to_csv("/content/data/test_images/output.txt", index = False)

!zip -r /content/data/test_images/input.zip /content/data/training_images/input
#!zip -r /content/data/test_images/output.zip /content/data/training_images/output
"""

!rm -r /content/data/test_images/input
!rm -r /content/data/test_images/output

import math
print(math.factorial(20000))





print("hej med dig jeg hedder Kaj")

print("OK HVAD MED DIG?")

print(2**20 * 2**200)

print("hallo")

