{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import re\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn.functional import relu\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchsummary import summary\n",
    "from sklearn.utils import shuffle\n",
    "from SHG import SHG\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PATHS FOR DATA\"\"\"\n",
    "\n",
    "# Input images\n",
    "TRAIN_IMGS_PATH = \"C:/Users/André/OneDrive 2/OneDrive/Skrivebord/bsc_data/train/image/\"\n",
    "VAL_IMGS_PATH = \"C:/Users/André/OneDrive 2/OneDrive/Skrivebord/bsc_data/validation/image/\"\n",
    "\n",
    "# Ground truth heatmaps\n",
    "TRAIN_HEATMAPS_PATH = \"C:/Users/André/OneDrive 2/OneDrive/Skrivebord/bsc_data/train/heatmaps/\"\n",
    "VAL_HEATMAPS_PATH = \"C:/Users/André/OneDrive 2/OneDrive/Skrivebord/bsc_data/validation/heatmaps/\"\n",
    "\n",
    "# Used saved model\n",
    "SAVED_MODEL_PATH = \"C:/Users/André/OneDrive 2/OneDrive/Skrivebord/bsc_data/models/Thu_Apr__1_20-36-35_2021/epoch_12.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-fbea42e6b84e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# If we decide to use a saved model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mSAVED_MODEL_PATH\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSAVED_MODEL_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# loads the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mstart_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"(?<=epoch_)(.*)(?=.pth)\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSAVED_MODEL_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m# finds the starting epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mcur_model_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"^(.*)(?=epoch)\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSAVED_MODEL_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# finds the directory of the saved model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    592\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m    851\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 853\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m    843\u001b[0m         \u001b[0mdata_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 845\u001b[1;33m             \u001b[0mload_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    846\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[1;34m(data_type, size, key, location)\u001b[0m\n\u001b[0;32m    831\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m         \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m         \u001b[0mloaded_storages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cur_model_path = None\n",
    "start_epoch = 0\n",
    "train_loss = np.array([])\n",
    "val_loss = np.array([])\n",
    "val_acc = np.array([])\n",
    "scheduler = None\n",
    "\n",
    "# Read the mean rgb if it has been calculated previously\n",
    "try:\n",
    "    average_rgb = np.loadtxt(\"./average_rgb.npy\")\n",
    "except:\n",
    "    average_rgb = get_mean_rgb(TRAIN_IMGS_PATH)\n",
    "    np.savetxt(\"./average_rgb.npy\", average_rgb)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Define model\n",
    "model = SHG(num_hourglasses=1).to(device)\n",
    "\n",
    "# If we decide to use a saved model\n",
    "if SAVED_MODEL_PATH is not None:\n",
    "    model.load_state_dict(torch.load(SAVED_MODEL_PATH)) # loads the model\n",
    "    start_epoch = int(re.findall(\"(?<=epoch_)(.*)(?=.pth)\", SAVED_MODEL_PATH)[0]) + 1 # finds the starting epoch\n",
    "    cur_model_path = re.findall(\"^(.*)(?=epoch)\", SAVED_MODEL_PATH)[0] # finds the directory of the saved model\n",
    "    train_loss = np.load(cur_model_path + \"loss.npy\") # loads the average training loss\n",
    "    val_loss = np.load(cur_model_path + \"val_loss.npy\") # loads the validation loss\n",
    "    scheduler = torch.load(cur_model_path + \"scheduler.pth\") # loads scheduler\n",
    "    val_acc = np.load(cur_model_path + \"val_acc.npy\") # loads validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (cur_model_path is None): # if we do not use a saved model\n",
    "    cur_model_path = \"C:/Users/André/OneDrive 2/OneDrive/Skrivebord/bsc_data/models/\" + time.asctime().replace(\" \", \"_\").replace(\":\", \"-\") + \"/\"\n",
    "    os.mkdir(cur_model_path)\n",
    "    print(\"Created directory at\", cur_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, X_path, y_path, average_rgb):\n",
    "        self.X_path = X_path\n",
    "        self.y_path = y_path\n",
    "        self.X_data = os.listdir(self.X_path)\n",
    "        self.average_rgb = average_rgb\n",
    "        self.norm = transforms.Normalize(mean = self.average_rgb, std = [1, 1, 1])\n",
    "        self.order = ['0.npy', '1.npy', '10.npy', '11.npy', '12.npy', '13.npy', '14.npy', '15.npy', '16.npy', '2.npy', '3.npy', '4.npy', '5.npy', '6.npy', '7.npy', '8.npy', '9.npy']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        ID = self.X_data[i]\n",
    "        x = Image.open(self.X_path + ID)\n",
    "\n",
    "        y = []\n",
    "\n",
    "        for i in range(17):\n",
    "            y.append(torch.from_numpy(np.load(self.y_path + ID[:-4] + \"/\" + str(i) + \".npy\")))\n",
    "\n",
    "        x = TF.to_tensor(x)\n",
    "\n",
    "        if (x.shape[0] == 1): # If the image is gray-scale, cast it to rgb\n",
    "            x = torch.stack((x[0],) * 3)\n",
    "\n",
    "        x = self.norm(x) # Subtracts mean rgb\n",
    "\n",
    "        y = torch.stack(y)\n",
    "        return x, y\n",
    "\n",
    "train_data = dataset(TRAIN_IMGS_PATH, TRAIN_HEATMAPS_PATH, average_rgb)\n",
    "val_data = dataset(VAL_IMGS_PATH, VAL_HEATMAPS_PATH, average_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2.5e-4 if scheduler is None else scheduler.state_dict()[\"_last_lr\"][0]\n",
    "print(\"USING LR = {}\".format(LEARNING_RATE))\n",
    "NUM_EPOCHS = 100\n",
    "MINI_BATCH_SIZE = 16\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "if (scheduler is None):\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.2, patience=5, mode = \"max\", eps = 2.5e-4 * 0.2, verbose=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size = MINI_BATCH_SIZE, shuffle = True)\n",
    "val_dataloader = DataLoader(val_data, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in tqdm(range(start_epoch, NUM_EPOCHS), desc = \"EPOCH\"):\n",
    "    model.train()\n",
    "    train_loss = np.append(train_loss, 0)\n",
    "    for x, y in tqdm(train_dataloader, leave = False, desc = \"MINI BATCH\", total = len(train_dataloader)):\n",
    "        x = x.to(device, dtype = torch.float)\n",
    "        y = y.to(device, dtype = torch.float)\n",
    "\n",
    "        # Predict\n",
    "        predictions, _ = model(x)\n",
    "\n",
    "        # Backpropegation\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Computes loss\n",
    "        loss = criterion(predictions.to(device), y)\n",
    "\n",
    "        # Store train loss\n",
    "        train_loss[-1] += loss.item()\n",
    "\n",
    "        # Backpropegation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss[-1] /= len(train_dataloader)\n",
    "    \n",
    "    print(\"Average train loss of epoch {}: {}\".format(epoch, train_loss[-1]))\n",
    "\n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss = np.append(val_loss, 0)\n",
    "        val_acc = np.append(val_acc, 0)\n",
    "        for x, y in tqdm(val_dataloader, leave = False, desc = \"VALIDATION\", total = len(val_dataloader)):\n",
    "            x = x.to(device, dtype = torch.float)\n",
    "            y = y.to(device, dtype = torch.float)\n",
    "\n",
    "            # Predict\n",
    "            predictions, _ = model(x)\n",
    "\n",
    "            # Loss\n",
    "            loss = criterion(predictions, y)\n",
    "\n",
    "            # Storing validation loss\n",
    "            val_loss[-1] += loss.item()\n",
    "\n",
    "            # Storing validation accuracy\n",
    "            val_acc[-1] += PCK(y.cpu(), predictions.cpu())\n",
    "\n",
    "        val_loss[-1] /= len(val_dataloader)\n",
    "        val_acc[-1] /= len(val_dataloader)\n",
    "\n",
    "    print(\"Validation loss at epoch {}: {}\".format(epoch, val_loss[-1]))\n",
    "    print(\"Validation accuracy at epoch {}: {}\".format(epoch, val_acc[-1]))\n",
    "\n",
    "    # Saving model\n",
    "    torch.save(model.state_dict(), cur_model_path + \"/epoch_{}\".format(epoch) + \".pth\")\n",
    "\n",
    "    # Saving training loss\n",
    "    np.save(cur_model_path + \"loss.npy\", train_loss)\n",
    "\n",
    "    # Saving validation loss\n",
    "    np.save(cur_model_path + \"val_loss.npy\", val_loss)\n",
    "\n",
    "    # Saving validation acc\n",
    "    np.save(cur_model_path + \"val_acc.npy\", val_acc)\n",
    "\n",
    "    # Scheduler\n",
    "    scheduler.step(val_acc[-1])\n",
    "    torch.save(scheduler, cur_model_path + \"scheduler.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# Skal måske sørge for, at LR kun plateauer én gang - tror jeg har sørget for, ved at gøre brug af \"eps\"\n",
    "# Skal måske implementere accuracy og køre den under træning ligesom ved validation\n",
    "# Hvis det går helt galt, kan jeg prøve kun at bruge keypoints hvor v = 2 (ligesom camilla gør)"
   ]
  }
 ]
}