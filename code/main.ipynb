{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python382jvsc74a57bd015b452054a1f37423623dc03587790452578e39807b086c75f037ab8f1c9c424",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import re\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn.functional import relu\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchsummary import summary\n",
    "from sklearn.utils import shuffle\n",
    "from SHG import SHG\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PATHS FOR DATA\"\"\"\n",
    "\n",
    "# Input images\n",
    "TRAIN_IMGS_PATH = \"C:/Users/André/OneDrive 2/OneDrive/Skrivebord/bsc_data/train/image/\"\n",
    "VAL_IMGS_PATH = \"C:/Users/André/OneDrive 2/OneDrive/Skrivebord/bsc_data/validation/image/\"\n",
    "\n",
    "# Ground truth heatmaps\n",
    "TRAIN_HEATMAPS_PATH = \"C:/Users/André/OneDrive 2/OneDrive/Skrivebord/bsc_data/train/heatmaps/\"\n",
    "VAL_HEATMAPS_PATH = \"C:/Users/André/OneDrive 2/OneDrive/Skrivebord/bsc_data/validation/heatmaps/\"\n",
    "\n",
    "# Used saved model\n",
    "SAVED_MODEL_PATH = \"C:/Users/André/OneDrive 2/OneDrive/Skrivebord/bsc_data/models/Mon_Apr_12_18-11-59_2021/epoch_46.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cur_model_path = None\n",
    "start_epoch = 0\n",
    "train_loss = np.array([])\n",
    "val_loss = np.array([])\n",
    "val_acc = np.array([])\n",
    "scheduler = None\n",
    "optimizer = None\n",
    "\n",
    "# Read the mean rgb if it has been calculated previously\n",
    "try:\n",
    "    average_rgb = np.loadtxt(\"./average_rgb.npy\")\n",
    "except:\n",
    "    average_rgb = get_mean_rgb(TRAIN_IMGS_PATH)\n",
    "    np.savetxt(\"./average_rgb.npy\", average_rgb)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Define model\n",
    "model = SHG(num_hourglasses=1).to(device)\n",
    "\n",
    "# If we decide to use a saved model\n",
    "if SAVED_MODEL_PATH is not None:\n",
    "    model.load_state_dict(torch.load(SAVED_MODEL_PATH)) # loads the model\n",
    "    start_epoch = int(re.findall(\"(?<=epoch_)(.*)(?=.pth)\", SAVED_MODEL_PATH)[0]) + 1 # finds the starting epoch\n",
    "    cur_model_path = re.findall(\"^(.*)(?=epoch)\", SAVED_MODEL_PATH)[0] # finds the directory of the saved model\n",
    "    train_loss = np.load(cur_model_path + \"loss.npy\") # loads the average training loss\n",
    "    val_loss = np.load(cur_model_path + \"val_loss.npy\") # loads the validation loss\n",
    "    scheduler = torch.load(cur_model_path + \"scheduler.pth\") # loads scheduler\n",
    "    val_acc = np.load(cur_model_path + \"val_acc.npy\") # loads validation accuracy\n",
    "    optimizer = torch.load(cur_model_path + \"optimizer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (cur_model_path is None): # if we do not use a saved model\n",
    "    cur_model_path = \"C:/Users/André/OneDrive 2/OneDrive/Skrivebord/bsc_data/models/\" + time.asctime().replace(\" \", \"_\").replace(\":\", \"-\") + \"/\"\n",
    "    os.mkdir(cur_model_path)\n",
    "    print(\"Created directory at\", cur_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, X_path, y_path, average_rgb):\n",
    "        self.X_path = X_path\n",
    "        self.y_path = y_path\n",
    "        self.X_data = os.listdir(self.X_path)\n",
    "        self.average_rgb = average_rgb\n",
    "        self.norm = transforms.Normalize(mean = self.average_rgb, std = [1, 1, 1])\n",
    "        self.order = ['0.npy', '1.npy', '10.npy', '11.npy', '12.npy', '13.npy', '14.npy', '15.npy', '16.npy', '2.npy', '3.npy', '4.npy', '5.npy', '6.npy', '7.npy', '8.npy', '9.npy']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        ID = self.X_data[i]\n",
    "        x = Image.open(self.X_path + ID)\n",
    "\n",
    "        y = []\n",
    "\n",
    "        for i in range(17):\n",
    "            y.append(torch.from_numpy(np.load(self.y_path + ID[:-4] + \"/\" + str(i) + \".npy\")))\n",
    "\n",
    "        x = TF.to_tensor(x)\n",
    "\n",
    "        if (x.shape[0] == 1): # If the image is gray-scale, cast it to rgb\n",
    "            x = torch.stack((x[0],) * 3)\n",
    "\n",
    "        x = self.norm(x) # Subtracts mean rgb\n",
    "\n",
    "        y = torch.stack(y)\n",
    "        return x, y\n",
    "\n",
    "train_data = dataset(TRAIN_IMGS_PATH, TRAIN_HEATMAPS_PATH, average_rgb)\n",
    "val_data = dataset(VAL_IMGS_PATH, VAL_HEATMAPS_PATH, average_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2.5e-4 if scheduler is None else scheduler.state_dict()[\"_last_lr\"][0]\n",
    "print(\"USING LR = {}\".format(LEARNING_RATE))\n",
    "NUM_EPOCHS = 100\n",
    "MINI_BATCH_SIZE = 16\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "if (scheduler is None):\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr = LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.2, patience=5, mode = \"max\", min_lr = 2.5e-4 * 0.2, verbose=True)\n",
    "    \n",
    "train_dataloader = DataLoader(train_data, batch_size = MINI_BATCH_SIZE, shuffle = True)\n",
    "val_dataloader = DataLoader(val_data, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in tqdm(range(start_epoch, NUM_EPOCHS), desc = \"EPOCH\"):\n",
    "    model.train()\n",
    "    train_loss = np.append(train_loss, 0)\n",
    "    print(\"EPOCH {}\".format(epoch))\n",
    "    for x, y in tqdm(train_dataloader, leave = False, desc = \"MINI BATCH\", total = len(train_dataloader)):\n",
    "        x = x.to(device, dtype = torch.float)\n",
    "        y = y.to(device, dtype = torch.float)\n",
    "\n",
    "        # Predict\n",
    "        predictions, _ = model(x)\n",
    "\n",
    "        # Backpropegation\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Computes loss\n",
    "        loss = criterion(predictions.to(device), y)\n",
    "\n",
    "        # Store train loss\n",
    "        train_loss[-1] += loss.item()\n",
    "\n",
    "        # Backpropegation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss[-1] /= len(train_dataloader)\n",
    "    \n",
    "    print(\"     Average train loss: {}\".format(train_loss[-1]))\n",
    "\n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss = np.append(val_loss, 0)\n",
    "        val_acc = np.append(val_acc, 0)\n",
    "        for x, y in tqdm(val_dataloader, leave = False, desc = \"VALIDATION\", total = len(val_dataloader)):\n",
    "            x = x.to(device, dtype = torch.float)\n",
    "            y = y.to(device, dtype = torch.float)\n",
    "\n",
    "            # Predict\n",
    "            predictions, _ = model(x)\n",
    "\n",
    "            # Loss\n",
    "            loss = criterion(predictions, y)\n",
    "\n",
    "            # Storing validation loss\n",
    "            val_loss[-1] += loss.item()\n",
    "\n",
    "            # Storing validation accuracy\n",
    "            val_acc[-1] += PCK(y.cpu(), predictions.cpu())\n",
    "\n",
    "        val_loss[-1] /= len(val_dataloader)\n",
    "        val_acc[-1] /= len(val_dataloader)\n",
    "\n",
    "    print(\"     Validation loss: {}\".format(val_loss[-1]))\n",
    "    print(\"     Validation accuracy: {}\".format(val_acc[-1]))\n",
    "\n",
    "    # Saving model\n",
    "    torch.save(model.state_dict(), cur_model_path + \"/epoch_{}\".format(epoch) + \".pth\")\n",
    "\n",
    "    # Saving training loss\n",
    "    np.save(cur_model_path + \"loss.npy\", train_loss)\n",
    "\n",
    "    # Saving validation loss\n",
    "    np.save(cur_model_path + \"val_loss.npy\", val_loss)\n",
    "\n",
    "    # Saving validation acc\n",
    "    np.save(cur_model_path + \"val_acc.npy\", val_acc)\n",
    "\n",
    "    # Saving optimizer\n",
    "    torch.save(optimizer, cur_model_path + \"optimizer.pth\")\n",
    "\n",
    "    # Scheduler\n",
    "    scheduler.step(val_acc[-1])\n",
    "    torch.save(scheduler, cur_model_path + \"scheduler.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# Hvis det går helt galt, kan jeg prøve kun at bruge keypoints hvor v = 2 (ligesom camilla gør)"
   ]
  }
 ]
}