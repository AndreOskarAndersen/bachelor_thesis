1. meeting
* https://github.com/tensorflow/tfjs-models/tree/master/posenet
* https://cocodataset.org/#home
* 2D articulated human pose estimation og hvad lærer modellen? ("Hvad lærer modellen" skal være lidt ekstra - altså, krøllen på halen)
* Brug scholar.google.com til at kigge frem i tiden
* Survey er papirer der sammenligner metoder
* Autoencoder med clustering og pca på midter lag (tror det hed bottleneck) kan bruges til at finde ud af hvad et et CNN lærer
* https://www.dropbox.com/sh/xden54w9z4imfld/AACVStkQSm-NV4EVIV65jwBBa?dl=0

* Kig på stacked hourglass
* Læs om autoencoder
* Skriv projektbeskrivelse (1. problem/motivation, 2. Dataset, 3. Projektplan/tidsplan, 4. projektspecifikke læringsmål)
* Læs Newell og Papendreou papers
* Kig på Michelles master thesis om pose estimation (tror det var noget med ballet)

Forberedelse til møde 2:
* Anvendelse af google collab (eller lignende)?
* Hvilken model skal bruges? Er ret glad for stacked hour glass
* Hvad skal jeg gøre med beskrivelsen af datasættet, projektets forbindelse til datalogi osv...
* Holder man ferie?
* Gode kilder på preprocessing af billeder som data
* Hvilket sprog samt bibliotek?

2. meeting:
* Behøver ikke være verdens bedste model - det er vigtigere den kan skildes ad
* Måske både giv billede og skelet som input/output til autoencoder
* Stacked hour glass: plus er concat, box er convolution (+ max pooling)
* EVERYTHING YOU NEED TO LEARN ABOUT DEEP LEARNING FOR COMPUTER VISION (læs frem til autoencover (og måske inklusiv))
* scikit image (vær opmærksom på bgr istedet for rgb)
* Kig på andre modellen inden Stacked hour glass inden jeg låser mig fast
* Skriv noget ned om COCO når jeg begynder at kigge på det, som så skal skrives ind i rapporten.
* Skriv noget ned om de artikler jeg har læst om, som så smides ind i indledningen. Skriv den sammenhængende valg af metode. "Der findes de her artikler ..., jeg har valgt at tage udgangspunkt i denne her artikel fordi...". Måske endda skriv dette allerede nu

Forberedelse til møde 3:
* For at gøre brug af SHG (eller en anden single-human model) skal der kun være én person i hvert billede - i COCO er der flere, hvordan gør jeg dette? Skal personen være i midten af billedet? Jeg har croppet billedet, så billedet tilpasses efter keypointsne
* Keypoints'ne består af 3 punkter: x, y, v, hvor v viser om keypointet er visible. Skal v også bruges til at træne modelen?
* Flere af de artikler jeg har læst flipper, roterer, rescaler, tilfører støj til deres billeder, burder jeg også gøre det?
* Hver kropsdel er ikke en enkel pixel i virkeligheden, men det er det i datasættet. Skal jeg tilføje en radius rundt om den "rigtige" pixel, som så fungerer som et "korrekt område"?
* Hvis en artikel har fundet en måde der bygger videre på SHG og forbedrer det, er det noget jeg burde overveje at gøre?
    * Ved godt det ikke behøver være verdens bedste model, men kunne det ikke være fedt at vise i mit projekt, at jeg har læst videre på den oprindelige idet. "Newell gør det på denne her måde, men det har vist sig at være bedere hvis man gør det på denne her måde..." - eller måske kan dette måske bruges når jeg skal forbedre min model efter XAI?
* "Related work"-afsnit i projekt?
* Hvor meget inspiration må jeg tage fra Camillas rapport?

Forberedelse til møde 4:
* Kontrakt er blevet godkendt
* Når jeg laver referencer til en artikle e.l., skal jeg så skrive hele navnet og alle navne eller et "Newell et. al" fint?
* Skal jeg referere til COCO hjemmesiden eller artiklen fra google scholar?
* Skal jeg skrive sidetal/sektion ned, når jeg laver referencer til artiker?
* Når jeg skriver at jeg bruger eksempelvis pytorch, skal jeg så referere til det? eller må jeg antage at læserne kender til det?
* Hvorfor sjove farver i rapport?
* Kan jeg skrive, at heatmaps fungerer som en "sandsynlighed" for hvor et led er?
* Skal jeg skrive hvordan man får dataen? eller er det fint at skrive, at jeg bruger COCO og så må læserne selv finde ud af hvordan man får det?
* Jeg udvider pt hver bounding box med 50 px. Burde jeg gøre, så det istedet afhænger af størrelsen af bounding boxen?
* Bedre måde at gemme preprocessed billeder på end png?
(https://stackoverflow.com/a/41425878/12905157)
* Skæv fordeling af validation og testing? (testing er ca. 5 gange så stort som validation)
* Hvilke activation function ville være passende?
* Hvad menes der med "After reaching the output resolution of the network, two consecutive rounds of 1x1 convolutions are applied to produce the final network predictions."
* For hver branch er der kun én convolution eller har de bare kun tegnet én?

Forberedelse til møde 5:
* Laver jeg backpropagation rigtigt? (samler loss for hvert hourglass i en liste, laver forward prop, kører så over listen og laver backprop., på hver loss)
* Ret langsom: 4 sek pr. feedforward - Newell bruger til sammenligning 75 ms
* Maxpooling på kernel_size = 60 tager meget lang tid

* SHG, fig 4: betyder 128x1x1 virkelig 128 convolutions af størrelse 1x1?
* De skriver "Filters greater than 3x3 are never used" - dette gælder både for maxpooling og convolutions?
* Er den blå boks en convolution (det ligner det ved camillas) 

* Hvordan ved jeg hvilke heatmaps der svarer til de rigtige? (ved beregning as loss)
* Jeg havde tænkt mig, at gøre forskel på heatmaps hvor et led er synligt og hvor ledet ikke er synligt, men det gør Newell ikke - skal jeg så også droppe det?
* Newell bruger 8 hourglasses, men Camilla bruger kun 2?
* Skal der altid en activation efter en convolution? - I tvivl om hvor activation functions skal være. Har bare fulgt Camillas
* Residuals/convolutions downsampler også en lille smule - er det meningen eller skal jeg justere dem, så de ikke gør?
* Nogle komponenter passer ikke sammen med den information der er givet. Kan det passe at noget information (padding, stride, kernel size) ikke er givet så man skal selv justere på dette indtil at komponenterne passer sammen?
* Hvorfor har output egentlig en size på 64x64?
* Hvordan laver jeg disse heatmaps på 64x64 (skal jeg bare downsample dem på 256x256)?

* Er én "feature" én pixel?
* Er "convolution", "kernel" og "filter" det samme? - Det lille gitter med tal.

Forberedelse til møde 6:
1. Man kan ikke altid gøre, så en person er i midten af det cropped billede. Skal jeg bare discarde disse personer eller skal jeg beholde dem?
2. Lærer modellen bare af sig selv, at den skal fokusere på personen i midten af billedet?
3. Hvorfor er det en god ide at trække den gennemsnitlige RGB fra alle billeder?
4. Skal jeg også referere til hvor jeg har noget viden fra?
5. Hvor mange epochs? (Newell bruger 4000 * 20 = 80.000 - fig. 8)
6. Kan jeg validere modellen efter hver x'te epoch, så jeg får en flot graf som i Fig. 8. eller kommer det til at tage for lang tid?
7. "Evaluation is done using the standard Percentage of Correct Keypoints (PCK) metric which reports the percentage of detections that fall within a normalized distance of the ground truth." - er det bare, at hvis predictionen ligger inden for en bestemt radius af groundtruth, så er den "korrekt" eller hvordan?
8. "For generating final test predictions we run both the original input and a flipped version of the image through the network and average the heatmaps together (accounting for a 1% average improvement on validation)." - Burde jeg også gøre dette?
9. "To improve performance at high precision thresholds the prediction is offset by a quarter of a pixel in the direction of its next highest neighbor before transforming back to the original coordinate space of the image. " - forstår jeg ikke helt

Forberedelse til møde 7:
1. Idet jeg preprocesser min data på samme måde som Newell gør, skal jeg så refere til ham/skrive at jeg gør på samme måde?
2. Hvis jeg skal refere til en bog online, skal jeg så refere til url'en hvor jeg har læst eller til selve bogen?
3. Hvordan laver jeg referencer til en forelæsningsslide?
4. Må jeg bruge youtube videoer som referencer?
5. Hvis jeg i et afsnit heletiden bruger skiftevis til to forskellige materialer, er det så fint nok hvis jeg bare tilsidst refererer til de to, istedet for at gøre det hver gang jeg skifter min anvendelse?

6. Mine inputbilleder ligger i range [0, 1] istedet for [0, 255]. Er det et problem nu hvor jeg kun gør brug af ReLu?

7. Accuracy metrics?
8. Er det rigtigt at bruge MSE også til validation? 
9. Når man laver validation, så skal mini-batch størrelsen være 1, right?
10. Når man laver validation så er det gennemsnittet af loss på validation sættet som fungerer som den samlede loss?
11. Modellen lærer kun at predicte 0'er
12. Mine probabilities for pixelsne er ofte negative og bare generelt meget lave
13. Allerede efter bare én epoch er min validation loss virkelig lav (10^(-5))

14. Skal jeg også trække gennemsnitlig RGB fra validation og test?
15. Kan man kalde det at "centralisere rgb" når man trækker den gennemsnitlige rgb fra?

16. Skal man clone ved branches?
17. Activation er en convolution og ikke før en convolution?
18. Er der bestemte steder for batch norms?

Forberedelse til møde 8:
1. Køretid er kommet ned på 1 time. Se loss udvikling - starter allerede ved epoch 0 med at være meget lav
2. Er det rigtigt, at vægtene ikke må starte med at være 0?
3. Camilla gør brug af Glorot normal distribution til at initializere sine vægte - skal jeg også det?
4. Hvad hvis jeg selv kommer op med noget?
5. Lavede gradient descent algoritme
6. Har skrevet lidt om på universal approximation theorem - Kan approximere alle Borel målelig function. Istedet: any continuous 
	function on a closed and bounded subset of R^n
7. Laver formler om
8. Kan man kalde det at "centralisere rgb" når man trækker den gennemsnitlige rgb fra?
9. Skal jeg skrive "I", "we" eller holde det mere generelt som "one/you"?
10. Hvis jeg bruger et stykke materiale, som referer til et andet stykke materiale, hvem skal jeg så refere til?
11. pt. bruger jeg rmsprop uden momentum - tror du jeg burde prøve med?

Forberedelse til møde 9:
1. Går jeg for matematisk til den?
2. Skal jeg beskrive intermediate supervision når jeg beskriver SHG, selvom jeg ikke bruger det i min implementation?
3. Må jeg kalde det "encoder" og "decoder"?
4. Må jeg kalde det at "downsample" når man maxpooler?
5. Skal jeg også skrive et al. ved Camilla, selvom hun kun er én person? Skal jeg kun skrive hendes efternavn?
6. Hvilke felter skulle jeg bruge når jeg skulle referere?
7. Normalt gør man brug af en baseline og flere eksperimenterer med flere forskellige modeller. Burde jeg også det?
8. Ifølge Newell er der stor forskel størrelsen af det ene hourglass (8 residuals er bedre end 8). Burde jeg have testet forskellige størrelser? (se fig. 8)
9. Hvorfor var det, at vægtene ikke måtte være = 0 ved start?

Forberedelse til møde 10:
* Pre MAD mail
1. Må jeg bruge mit testset til at finde ud af hvilken model jeg skal bruge til xai (mindste los != bedste accuracy)
2. Hvad med, at sende gennemsnitligt billeder for hvert cluster igennem modellen
3. Se på gennemsnitligt billed for hvert led den har fået korrekt/forkert (dvs. i alt 2*17 billeder)
4. Nearest neighbours
5. Der er vel to forskellige måder man kan gå til XAI? Kigge på modellen og kigge på dataen? Man kan vel lave en masse statistik på de to
grupper (dem den har fået rigtigt og dem den har fået forkert)
